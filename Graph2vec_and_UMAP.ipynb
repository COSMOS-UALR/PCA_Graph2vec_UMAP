{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MzDmJEonkFAy"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import List\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from karateclub.utils.treefeatures import WeisfeilerLehmanHashing\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "from scipy.spatial.distance import cdist\n",
    "import pymysql\n",
    "import base64\n",
    "from itertools import combinations\n",
    "import umap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, cophenet, dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cijuibzQAhsK"
   },
   "source": [
    "**Graph2vec and UMAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HC-9AODIAlQw"
   },
   "outputs": [],
   "source": [
    "class Graph2Vec:\n",
    "    def __init__(self, wl_iterations: int = 2, attributed: bool = False, dimensions: int = 128,\n",
    "                 workers: int = 4, down_sampling: float = 0.0001, epochs: int = 10, learning_rate: float = 0.025,\n",
    "                 min_count: int = 5, seed: int = 42, erase_base_features: bool = False):\n",
    "        self.wl_iterations = wl_iterations\n",
    "        self.attributed = attributed\n",
    "        self.dimensions = dimensions\n",
    "        self.workers = workers\n",
    "        self.down_sampling = down_sampling\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_count = min_count\n",
    "        self.seed = seed\n",
    "        self.erase_base_features = erase_base_features\n",
    "\n",
    "    def _set_seed(self):\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def fit(self, graphs: List[nx.classes.graph.Graph]):\n",
    "        self._set_seed()\n",
    "        documents = [\n",
    "            WeisfeilerLehmanHashing(\n",
    "                graph, self.wl_iterations, self.attributed, self.erase_base_features\n",
    "            )\n",
    "            for graph in graphs\n",
    "        ]\n",
    "        documents = [\n",
    "            TaggedDocument(words=doc.get_graph_features(), tags=[str(i)])\n",
    "            for i, doc in enumerate(documents)\n",
    "        ]\n",
    "\n",
    "        self.model = Doc2Vec(\n",
    "            documents,\n",
    "            vector_size=self.dimensions,\n",
    "            window=0,\n",
    "            min_count=self.min_count,\n",
    "            dm=0,\n",
    "            sample=self.down_sampling,\n",
    "            workers=self.workers,\n",
    "            epochs=self.epochs,\n",
    "            alpha=self.learning_rate,\n",
    "            seed=self.seed,\n",
    "        )\n",
    "\n",
    "        self._embedding = [self.model.docvecs[str(i)] for i, _ in enumerate(documents)]\n",
    "\n",
    "    def get_embedding(self) -> np.array:\n",
    "        return np.array(self._embedding)\n",
    "\n",
    "    def infer(self, graphs) -> np.array:\n",
    "        self._set_seed()\n",
    "        documents = [\n",
    "            WeisfeilerLehmanHashing(\n",
    "                graph, self.wl_iterations, self.attributed, self.erase_base_features\n",
    "            )\n",
    "            for graph in graphs\n",
    "        ]\n",
    "        documents = [TaggedDocument(words=doc.get_graph_features(), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
    "        return np.array([self.model.infer_vector(doc.words) for doc in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ns9qFzgsAroB"
   },
   "outputs": [],
   "source": [
    "class Graph2VecUMAP:\n",
    "    def __init__(self, graph2vec: Graph2Vec, n_neighbors: int = 5, min_dist: float = 0.1,\n",
    "                 n_components: int = 4, metric: str = 'euclidean', random_state: int = 42):\n",
    "        self.graph2vec = graph2vec\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.min_dist = min_dist\n",
    "        self.n_components = n_components\n",
    "        self.metric = metric\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit_transform(self, graphs: List[nx.classes.graph.Graph]):\n",
    "        graph2vec_embeddings = self.graph2vec.infer(graphs)\n",
    "        self.umap_model = umap.UMAP(\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            min_dist=self.min_dist,\n",
    "            n_components=self.n_components,\n",
    "            metric=self.metric,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        self.embedding = self.umap_model.fit_transform(graph2vec_embeddings)\n",
    "        return self.embedding\n",
    "\n",
    "    def transform(self, graphs: List[nx.classes.graph.Graph]):\n",
    "        graph2vec_embeddings = self.graph2vec.infer(graphs)\n",
    "        self.embedding = self.umap_model.transform(graph2vec_embeddings)\n",
    "        return self.embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "FMAmQO8MAsp0",
    "outputId": "18de0ce0-be7a-4236-ef5a-e3c8efbc3a43"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('airforce_channels_anomalous.csv')\n",
    "df2 = pd.read_csv('random_news_channels.csv')\n",
    "df_segmentation = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "df_segmentation = df_segmentation.fillna(0)\n",
    "df_segmentation.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzK57oS0AyRK"
   },
   "source": [
    "**Create graphs from gexf_files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eU_bSyHFA1u4",
    "outputId": "96da0f7c-6cce-4bb7-f9a8-759b3d3c7bf2"
   },
   "outputs": [],
   "source": [
    "def check_file_in_directory(directory_path, file_name):\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    return os.path.isfile(file_path)\n",
    "\n",
    "graphs = []\n",
    "gexf_files = []\n",
    "for i in list(df_segmentation['channel_id']):\n",
    "    if check_file_in_directory(\"processed_data\", f\"{i}.gexf\"):\n",
    "        gexf_files.append('processed_data/'+i+'.gexf')\n",
    "\n",
    "    else:\n",
    "        print(f\"{i}.gexf does not exist in processed_data\")\n",
    "j=0\n",
    "graphs = []\n",
    "graph_ids=[]\n",
    "for gexf_file in gexf_files:\n",
    "    graph = nx.read_gexf(gexf_file)\n",
    "    print(j,graph,gexf_file)\n",
    "    left_index = 15\n",
    "    right_index = -5\n",
    "    left_cut = gexf_file[:left_index]\n",
    "    right_cut = gexf_file[right_index:]\n",
    "    graph_id = gexf_file[left_index:right_index]\n",
    "    graph_ids.append(graph_id)\n",
    "    graphs.append(graph)\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWW015WyA6t3",
    "outputId": "0ebdec53-d46a-40e6-956c-ee6bfd96afa5"
   },
   "outputs": [],
   "source": [
    "graph2vec = Graph2Vec()\n",
    "graph2vec.fit(graphs)\n",
    "graph2vec_umap = Graph2VecUMAP(graph2vec, n_components=4)\n",
    "embedding = graph2vec_umap.fit_transform(graphs)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvBesjSRkpgH"
   },
   "source": [
    "**Optimal number of clusters for K-means clustering (silhouette score)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0fsUx3_kBwXf",
    "outputId": "15c93fd0-0a39-43e9-d8ef-ce6bd644082e"
   },
   "outputs": [],
   "source": [
    "def calculate_optimal_clusters(embedding, max_clusters=10):\n",
    "    silhouette_scores = []\n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        labels = kmeans.fit_predict(embedding)\n",
    "        score = silhouette_score(embedding, labels)\n",
    "        silhouette_scores.append((n_clusters, score))\n",
    "    optimal_n_clusters = max(silhouette_scores, key=lambda x: x[1])[0]\n",
    "    return optimal_n_clusters, silhouette_scores\n",
    "\n",
    "def visualize_clusters_3d(embedding, labels, n_clusters):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    colors = plt.cm.get_cmap('tab10', n_clusters)\n",
    "\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_points = embedding[labels == cluster]\n",
    "        ax.scatter(cluster_points[:, 0], cluster_points[:, 1], cluster_points[:, 2],\n",
    "                   label=f\"Cluster {cluster}\", s=50, alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel('UMAP Dimension 1', fontsize=10, labelpad=0)\n",
    "    ax.set_ylabel('UMAP Dimension 2', fontsize=10, labelpad=0)\n",
    "    ax.set_zlabel('UMAP Dimension 3', fontsize=10, labelpad=0)\n",
    "    ax.set_title(f\"KMeans Clustering\", fontsize=16, pad=20)\n",
    "\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    max_clusters = 10\n",
    "optimal_n_clusters, silhouette_scores = calculate_optimal_clusters(embedding, max_clusters)\n",
    "\n",
    "print(f\"Optimal number of clusters: {optimal_n_clusters}\")\n",
    "for n_clusters, score in silhouette_scores:\n",
    "    print(f\"Clusters: {n_clusters}, Silhouette Score: {score:.4f}\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(embedding)\n",
    "\n",
    "visualize_clusters_3d(embedding, labels, optimal_n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38XZS_NqBWUf"
   },
   "source": [
    "**Optimal number of clusters for Hierarchical Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "id": "pMg5cyaHBWv-",
    "outputId": "476b1b64-a2b3-4c1b-cee9-baec0ed2b8be"
   },
   "outputs": [],
   "source": [
    "def perform_hierarchical_clustering(data, max_clusters=10):\n",
    "    linkage_matrix = linkage(data, method=\"ward\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    dendrogram(linkage_matrix)\n",
    "    plt.title(\"Dendrogram\", fontsize=16)\n",
    "    plt.xlabel(\"Channel Index\", fontsize=16)\n",
    "    plt.ylabel(\"Distance\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    silhouette_scores = []\n",
    "    print(\"Evaluating Hierarchical clustering silhouette scores:\")\n",
    "    for k in range(2, max_clusters + 1):\n",
    "        labels = AgglomerativeClustering(n_clusters=k, linkage=\"ward\").fit_predict(data)\n",
    "        score = silhouette_score(data, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"Number of Clusters: {k}, Silhouette Score: {score:.4f}\")\n",
    "\n",
    "    optimal_k = 2 + np.argmax(silhouette_scores)\n",
    "    return optimal_k, silhouette_scores\n",
    "\n",
    "optimal_k, silhouette_scores = perform_hierarchical_clustering(embedding, max_clusters=10)\n",
    "print(f\"Optimal number of clusters based on silhouette scores: {optimal_k}\")\n",
    "\n",
    "Z = linkage(embedding, method='ward')\n",
    "\n",
    "max_d = 6 \n",
    "clusters = fcluster(Z, max_d, criterion='distance')\n",
    "n_clusters = len(set(clusters))\n",
    "\n",
    "c, coph_dists = cophenet(Z, pdist(embedding))\n",
    "print(\"Cophenetic Correlation Coefficient:\", c)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
